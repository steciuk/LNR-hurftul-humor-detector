{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from enum import Enum\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(os.getcwd(), 'data', 'train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename the columns to make them easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "      <th>woman</th>\n",
       "      <th>lgbtiq</th>\n",
       "      <th>race</th>\n",
       "      <th>gordofobia</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MENTION Ya estará colocada en algún chiringuit...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sin querer, hoy le dije “Hola” a una feminista...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>En este capítulo, que se transmitió en el Cana...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MENTION MENTION MENTION Concuerdo contigo en e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>– ¡No es no! ¡En mi cuerpo mando yo!\\r\\n– ¡Así...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  humor  woman  \\\n",
       "index                                                                    \n",
       "9      MENTION Ya estará colocada en algún chiringuit...      0      1   \n",
       "22     Sin querer, hoy le dije “Hola” a una feminista...      1      1   \n",
       "30     En este capítulo, que se transmitió en el Cana...      0      0   \n",
       "40     MENTION MENTION MENTION Concuerdo contigo en e...      0      0   \n",
       "45     – ¡No es no! ¡En mi cuerpo mando yo!\\r\\n– ¡Así...      1      1   \n",
       "\n",
       "       lgbtiq  race  gordofobia  mean  \n",
       "index                                  \n",
       "9           0     0           0   3.4  \n",
       "22          0     0           0   3.8  \n",
       "30          1     0           0   2.2  \n",
       "40          1     0           0   3.8  \n",
       "45          0     0           0   2.2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>humor</th>\n",
       "      <th>woman</th>\n",
       "      <th>lgbtiq</th>\n",
       "      <th>race</th>\n",
       "      <th>gordofobia</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2671.000000</td>\n",
       "      <td>2671.000000</td>\n",
       "      <td>2671.000000</td>\n",
       "      <td>2671.000000</td>\n",
       "      <td>2671.000000</td>\n",
       "      <td>2671.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.325346</td>\n",
       "      <td>0.483714</td>\n",
       "      <td>0.227256</td>\n",
       "      <td>0.248596</td>\n",
       "      <td>0.080120</td>\n",
       "      <td>3.053126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.468592</td>\n",
       "      <td>0.499828</td>\n",
       "      <td>0.419138</td>\n",
       "      <td>0.432280</td>\n",
       "      <td>0.271529</td>\n",
       "      <td>0.809173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             humor        woman       lgbtiq         race   gordofobia  \\\n",
       "count  2671.000000  2671.000000  2671.000000  2671.000000  2671.000000   \n",
       "mean      0.325346     0.483714     0.227256     0.248596     0.080120   \n",
       "std       0.468592     0.499828     0.419138     0.432280     0.271529   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              mean  \n",
       "count  2671.000000  \n",
       "mean      3.053126  \n",
       "std       0.809173  \n",
       "min       0.400000  \n",
       "25%       2.400000  \n",
       "50%       3.000000  \n",
       "75%       3.600000  \n",
       "max       5.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = pd.read_csv(TRAIN_PATH, index_col='index')\n",
    "data_df = data_df.rename(columns={\n",
    "    'tweet': 'text',\n",
    "    'prejudice_woman': 'woman',\n",
    "    'prejudice_lgbtiq': 'lgbtiq',\n",
    "    'prejudice_inmigrant_race': 'race',\n",
    "    'mean_prejudice': 'mean',\n",
    "})\n",
    "\n",
    "data_df = data_df.sort_index()\n",
    "\n",
    "display(data_df.head(5))\n",
    "display(data_df.describe())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_MODEL = \"es_core_news_sm\"\n",
    "# TODO: Try other spanish spacy models: es_core_news_md, es_core_news_lg, es_dep_news_trf\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(SPACY_MODEL)\n",
    "except OSError:\n",
    "    spacy.cli.download(SPACY_MODEL)\n",
    "    nlp = spacy.load(SPACY_MODEL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the default pipeline of the spacy model, we can get tokens with information about their part of speech, lemma, whether they are a stop word, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "      <th>woman</th>\n",
       "      <th>lgbtiq</th>\n",
       "      <th>race</th>\n",
       "      <th>gordofobia</th>\n",
       "      <th>mean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MENTION Ya estará colocada en algún chiringuit...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>[MENTION, Ya, estará, colocada, en, algún, chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sin querer, hoy le dije “Hola” a una feminista...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[Sin, querer, ,, hoy, le, dije, “, Hola, ”, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>En este capítulo, que se transmitió en el Cana...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>[En, este, capítulo, ,, que, se, transmitió, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MENTION MENTION MENTION Concuerdo contigo en e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[MENTION, MENTION, MENTION, Concuerdo, contigo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>– ¡No es no! ¡En mi cuerpo mando yo!\\r\\n– ¡Así...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>[–, ¡, No, es, no, !, ¡, En, mi, cuerpo, mando...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  humor  woman  \\\n",
       "index                                                                    \n",
       "9      MENTION Ya estará colocada en algún chiringuit...      0      1   \n",
       "22     Sin querer, hoy le dije “Hola” a una feminista...      1      1   \n",
       "30     En este capítulo, que se transmitió en el Cana...      0      0   \n",
       "40     MENTION MENTION MENTION Concuerdo contigo en e...      0      0   \n",
       "45     – ¡No es no! ¡En mi cuerpo mando yo!\\r\\n– ¡Así...      1      1   \n",
       "\n",
       "       lgbtiq  race  gordofobia  mean  \\\n",
       "index                                   \n",
       "9           0     0           0   3.4   \n",
       "22          0     0           0   3.8   \n",
       "30          1     0           0   2.2   \n",
       "40          1     0           0   3.8   \n",
       "45          0     0           0   2.2   \n",
       "\n",
       "                                                  tokens  \n",
       "index                                                     \n",
       "9      [MENTION, Ya, estará, colocada, en, algún, chi...  \n",
       "22     [Sin, querer, ,, hoy, le, dije, “, Hola, ”, a,...  \n",
       "30     [En, este, capítulo, ,, que, se, transmitió, e...  \n",
       "40     [MENTION, MENTION, MENTION, Concuerdo, contigo...  \n",
       "45     [–, ¡, No, es, no, !, ¡, En, mi, cuerpo, mando...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize text\n",
    "parsed_df = data_df.copy()\n",
    "parsed_df['tokens'] = list(nlp.pipe(parsed_df['text']))\n",
    "parsed_df['tokens'] = parsed_df['tokens'].apply(lambda x: [token for token in x])\n",
    "display(parsed_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  algún\n",
      "Lemma:  alguno\n",
      "POS:  DET\n",
      "Tag:  DET\n",
      "Dep:  det\n",
      "Shape:  xxxx\n",
      "Is alpha:  True\n",
      "Is stop:  True\n"
     ]
    }
   ],
   "source": [
    "# Show some data about a token\n",
    "token: Token = parsed_df['tokens'].iloc[0][5]\n",
    "print(\"Text: \", token.text)\n",
    "print(\"Lemma: \", token.lemma_)\n",
    "print(\"POS: \", token.pos_)\n",
    "print(\"Tag: \", token.tag_)\n",
    "print(\"Dep: \", token.dep_)\n",
    "print(\"Shape: \", token.shape_)\n",
    "print(\"Is alpha: \", token.is_alpha)\n",
    "print(\"Is stop: \", token.is_stop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token filtering utility\n",
    "Let's create utility functions to filter the tokens based on their attributes. We can easily use these functions to filter the tokens, build and experiment with different representations of the data in the next steps of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOJI_PATTERN = re.compile(\n",
    "    \"[\"\n",
    "    + \"\\U0001F600-\\U0001F64F\"\n",
    "    + \"\\U0001F300-\\U0001F5FF\"\n",
    "    + \"\\U0001F680-\\U0001F6FF\"\n",
    "    + \"\\U0001F1E0-\\U0001F1FF\"\n",
    "    + \"\\U00002500-\\U00002BEF\"\n",
    "    + \"\\U00002702-\\U000027B0\"\n",
    "    + \"\\U00002702-\\U000027B0\"\n",
    "    + \"\\U000024C2-\\U0001F251\"\n",
    "    + \"\\U0001f926-\\U0001f937\"\n",
    "    + \"\\U00010000-\\U0010ffff\"\n",
    "    + \"\\u2640-\\u2642\"\n",
    "    + \"\\u2600-\\u2B55\"\n",
    "    + \"\\u200d\"\n",
    "    + \"\\u23cf\"\n",
    "    + \"\\u23e9\"\n",
    "    + \"\\u231a\"\n",
    "    + \"\\ufe0f\"\n",
    "    + \"\\u3030\"\n",
    "    + \"]+\",\n",
    "    flags=re.UNICODE,\n",
    ")\n",
    "\n",
    "try:\n",
    "    STOPWORDS = nltk.corpus.stopwords.words(\"spanish\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "    STOPWORDS = nltk.corpus.stopwords.words(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_FUNC = {\n",
    "    \"punct\": lambda token: token.is_punct,\n",
    "    \"stopwords\": lambda token: token.text.lower() in STOPWORDS,\n",
    "    \"emoji\": lambda token: EMOJI_PATTERN.match(token.text),\n",
    "    \"number\": lambda token: token.like_num,\n",
    "    \"newline\": lambda token: re.match(r\"\\n+\", token.text),\n",
    "    \"space\": lambda token: token.is_space and not re.match(r\"\\n+\", token.text),\n",
    "    \"tags\": lambda token: token.text in [\"MENTION\", \"HASHTAG\", \"URL\"]\n",
    "}\n",
    "\n",
    "class Filter(Enum):\n",
    "    PUNCT = \"punct\"\n",
    "    STOPWORDS = \"stopwords\"\n",
    "    EMOJI = \"emoji\"\n",
    "    NUMBER = \"number\"\n",
    "    NEWLINE = \"newline\"\n",
    "    SPACE = \"space\"\n",
    "    TAGS = \"tags\"\n",
    "\n",
    "\n",
    "def filter_tokens(series: pd.Series, filters: list[Filter]) -> pd.Series:\n",
    "    '''\n",
    "    Filters a series of tokens using the given filters.\n",
    "    args:\n",
    "        series: a series of lists of tokens\n",
    "        filters: a list of filters to apply\n",
    "    returns:\n",
    "        a series of lists of tokens\n",
    "    '''\n",
    "    return series.apply(\n",
    "        lambda tokens: [token for token in tokens if not any([FILTER_FUNC[filter.value](token) for filter in filters])]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: '\n",
      "\n",
      "', got: '\n",
      "\n",
      "'\n",
      "Test passed!\n",
      "Expected: '\n",
      "', got: '\n",
      "'\n",
      "Expected: '\n",
      "', got: '\n",
      "'\n",
      "Expected: '\n",
      "', got: '\n",
      "'\n",
      "Expected: '\n",
      "', got: '\n",
      "'\n",
      "Expected: 'HASHTAG', got: '\n",
      "\n",
      "'\n",
      "Expected: '   ', got: 'HASHTAG'\n",
      "Expected: 'feminismo', got: '   '\n",
      "Expected: '  ', got: 'feminismo'\n",
      "Expected: '#', got: '  '\n",
      "Expected: '  \n",
      "', got: '  \n",
      "'\n",
      "Expected: '\n",
      "', got: '\n",
      "'\n",
      "Expected: '\n",
      "', got: '\n",
      "'\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_filter_tokens(result, expected):\n",
    "    error = False\n",
    "    for token, expected_token in zip(result, expected):\n",
    "        try:\n",
    "            assert token.text == expected_token\n",
    "        except AssertionError:\n",
    "            print(f\"Expected: '{expected_token}', got: '{token.text}'\")\n",
    "            error = True\n",
    "\n",
    "    if not error:\n",
    "        print(\"Test passed!\")\n",
    "\n",
    "# test Filter.PUNCT\n",
    "result = filter_tokens(parsed_df['tokens'], [Filter.PUNCT]).loc[22]\n",
    "expected = ['Sin', 'querer', 'hoy', 'le', 'dije', 'Hola', 'a', 'una', 'feminista', 'El', 'juicio', 'es', 'mañana', '\\n\\n', 'HASHTAG', '   ', 'feminismo', '  ']\n",
    "test_filter_tokens(result, expected)\n",
    "\n",
    "# test Filter.STOPWORDS\n",
    "result = filter_tokens(parsed_df['tokens'], [Filter.STOPWORDS]).loc[9]\n",
    "expected = ['MENTION', 'colocada', 'algún', 'chiringuito', 'feminazi']\n",
    "test_filter_tokens(result, expected)\n",
    "\n",
    "# test Filter.EMOJI\n",
    "result = filter_tokens(parsed_df['tokens'], [Filter.EMOJI]).loc[75]\n",
    "expected = ['Demasiadas', 'mujeres', ',', 'demasiadas', 'mujeres', '\\n', 'URL']\n",
    "test_filter_tokens(result, expected)\n",
    "\n",
    "# test Filter.NUMBER\n",
    "result = filter_tokens(parsed_df['tokens'], [Filter.NUMBER]).loc[5245]\n",
    "expected = ['Acá', 'te', 'lo', 'aclaro', ':', '\\n', 'de', 'octubre', ':', 'Día', 'de', 'brujas', '(', 'Mujeres', ')', '\\n', 'de', 'noviembre', ':', 'Día', 'de', 'todos', 'los', 'santos', '(', 'Hombres', ')', '\\n', 'noviembre', ':', 'Día', 'de', 'los', 'difuntos', '(', 'Hombres', 'que', 'se', 'animaron', 'a', 'decirle', 'brujas', 'a', 'las', 'mujeres', ')']\n",
    "test_filter_tokens(result, expected)\n",
    "\n",
    "# test Filter.NEWLINE\n",
    "result = filter_tokens(parsed_df['tokens'], [Filter.NEWLINE]).loc[22]\n",
    "expected = ['Sin', 'querer', ',', 'hoy', 'le', 'dije', '“', 'Hola', '”', 'a', 'una', 'feminista', '.', 'El', 'juicio', 'es', 'mañana', '.', 'HASHTAG', '   ', 'feminismo', '  ', '#']\n",
    "test_filter_tokens(result, expected)\n",
    "\n",
    "# test Filter.SPACE\n",
    "result = filter_tokens(parsed_df['tokens'], [Filter.SPACE]).loc[22]\n",
    "expected = ['Sin', 'querer', ',', 'hoy', 'le', 'dije', '“', 'Hola', '”', 'a', 'una', 'feminista', '.', 'El', 'juicio', 'es', 'mañana', '.', '\\n\\n', 'HASHTAG', 'feminismo', '#']\n",
    "\n",
    "# test Filter.TAGS\n",
    "result = filter_tokens(parsed_df['tokens'], [Filter.TAGS]).loc[14869]\n",
    "# FIXME: Notice that when there is no space between `-` and the next word, both are recognized as a single token\n",
    "expected = ['  ', '  ', '  \\n', '-Te', 'cuento', 'un', 'chiste', 'machista', '?', '\\n', '-Pero', 'que', 'dices', ',', 'si', 'soy', 'mujer', '.', '\\n', '-Tranquila', ',', 'te', 'lo', 'explico', '.']\n",
    "test_filter_tokens(result, expected)\n",
    "\n",
    "\n",
    "# test all\n",
    "result = filter_tokens(parsed_df['tokens'], [Filter.PUNCT, Filter.STOPWORDS, Filter.EMOJI, Filter.NUMBER, Filter.NEWLINE, Filter.SPACE]).loc[5245]\n",
    "expected = ['Acá', 'aclaro', 'octubre', 'Día', 'brujas', 'Mujeres', 'noviembre', 'Día', 'santos', 'Hombres', 'noviembre', 'Día', 'difuntos', 'Hombres', 'animaron', 'decirle', 'brujas', 'mujeres']\n",
    "test_filter_tokens(result, expected)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "TODO: Try different token filtering options for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "      <th>woman</th>\n",
       "      <th>lgbtiq</th>\n",
       "      <th>race</th>\n",
       "      <th>gordofobia</th>\n",
       "      <th>mean</th>\n",
       "      <th>tokens</th>\n",
       "      <th>processed</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MENTION Ya estará colocada en algún chiringuit...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>[MENTION, Ya, estará, colocada, en, algún, chi...</td>\n",
       "      <td>mention colocado alguno chiringuito feminazi</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sin querer, hoy le dije “Hola” a una feminista...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[Sin, querer, ,, hoy, le, dije, “, Hola, ”, a,...</td>\n",
       "      <td>querer hoy decir hola feminista juicio mañana ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>En este capítulo, que se transmitió en el Cana...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>[En, este, capítulo, ,, que, se, transmitió, e...</td>\n",
       "      <td>capítulo transmitir canal televisión británico...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MENTION MENTION MENTION Concuerdo contigo en e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[MENTION, MENTION, MENTION, Concuerdo, contigo...</td>\n",
       "      <td>mention mention mention concuerdo contigo supe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>– ¡No es no! ¡En mi cuerpo mando yo!\\r\\n– ¡Así...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>[–, ¡, No, es, no, !, ¡, En, mi, cuerpo, mando...</td>\n",
       "      <td>cuerpo mando así hablar empoderada poder abort...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  humor  woman  \\\n",
       "index                                                                    \n",
       "9      MENTION Ya estará colocada en algún chiringuit...      0      1   \n",
       "22     Sin querer, hoy le dije “Hola” a una feminista...      1      1   \n",
       "30     En este capítulo, que se transmitió en el Cana...      0      0   \n",
       "40     MENTION MENTION MENTION Concuerdo contigo en e...      0      0   \n",
       "45     – ¡No es no! ¡En mi cuerpo mando yo!\\r\\n– ¡Así...      1      1   \n",
       "\n",
       "       lgbtiq  race  gordofobia  mean  \\\n",
       "index                                   \n",
       "9           0     0           0   3.4   \n",
       "22          0     0           0   3.8   \n",
       "30          1     0           0   2.2   \n",
       "40          1     0           0   3.8   \n",
       "45          0     0           0   2.2   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "9      [MENTION, Ya, estará, colocada, en, algún, chi...   \n",
       "22     [Sin, querer, ,, hoy, le, dije, “, Hola, ”, a,...   \n",
       "30     [En, este, capítulo, ,, que, se, transmitió, e...   \n",
       "40     [MENTION, MENTION, MENTION, Concuerdo, contigo...   \n",
       "45     [–, ¡, No, es, no, !, ¡, En, mi, cuerpo, mando...   \n",
       "\n",
       "                                               processed  \\\n",
       "index                                                      \n",
       "9           mention colocado alguno chiringuito feminazi   \n",
       "22     querer hoy decir hola feminista juicio mañana ...   \n",
       "30     capítulo transmitir canal televisión británico...   \n",
       "40     mention mention mention concuerdo contigo supe...   \n",
       "45     cuerpo mando así hablar empoderada poder abort...   \n",
       "\n",
       "                                                  tf-idf  \n",
       "index                                                     \n",
       "9      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "22     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "30     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "40     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "45     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove stopwords, punctuations, emojis, numbers, newlines and spaces\n",
    "tf_idf_df = parsed_df.copy()\n",
    "tf_idf_df['processed'] = filter_tokens(tf_idf_df['tokens'], [Filter.PUNCT, Filter.STOPWORDS, Filter.EMOJI, Filter.NUMBER, Filter.NEWLINE, Filter.SPACE])\n",
    "\n",
    "# Use lemmas instead of tokens\n",
    "tf_idf_df['processed'] = tf_idf_df['processed'].apply(lambda tokens: [token.lemma_ for token in tokens])\n",
    "\n",
    "# Concat all tokens into a single string\n",
    "# This is needed for the TF-IDF vectorizer\n",
    "tf_idf_df['processed'] = tf_idf_df['processed'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# lowercase\n",
    "tf_idf_df['processed'] = tf_idf_df['processed'].apply(lambda tokens: tokens.lower())\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf_df[\"tf-idf\"] = tf_idf_vectorizer.fit_transform(tf_idf_df['processed']).toarray().tolist()\n",
    "\n",
    "display(tf_idf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tf-idf vector: 7875\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of tf-idf vector: {len(tf_idf_df['tf-idf'].iloc[0])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "### FastText Spanish Unannotated Corpora\n",
    "Pre-trained word embeddings were downloaded from [dccuchile/spanish-word-embeddings](https://github.com/dccuchile/spanish-word-embeddings#fasttext-embeddings-from-suc).\n",
    "\n",
    "According to [josecannete/spanish-corpora](https://github.com/josecannete/spanish-corpora) the corpus on which the FastText embeddings were trained was processed in the following way:\n",
    "\n",
    "> - Lowercase\n",
    "> - Removed urls\n",
    "> - Removed listing\n",
    "> - Replaced multiple spaces with single one\n",
    "\n",
    "so in order to get the best results we will need to do the same. Urls in the training set are replaced with the string `URL` but we will remove them anyway. We will do the same for hashtags and mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "      <th>woman</th>\n",
       "      <th>lgbtiq</th>\n",
       "      <th>race</th>\n",
       "      <th>gordofobia</th>\n",
       "      <th>mean</th>\n",
       "      <th>tokens</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MENTION Ya estará colocada en algún chiringuit...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>[MENTION, Ya, estará, colocada, en, algún, chi...</td>\n",
       "      <td>[Ya, estará, colocada, en, algún, chiringuito,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sin querer, hoy le dije “Hola” a una feminista...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[Sin, querer, ,, hoy, le, dije, “, Hola, ”, a,...</td>\n",
       "      <td>[Sin, querer, ,, hoy, le, dije, “, Hola, ”, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>En este capítulo, que se transmitió en el Cana...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>[En, este, capítulo, ,, que, se, transmitió, e...</td>\n",
       "      <td>[En, este, capítulo, ,, que, se, transmitió, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MENTION MENTION MENTION Concuerdo contigo en e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[MENTION, MENTION, MENTION, Concuerdo, contigo...</td>\n",
       "      <td>[Concuerdo, contigo, en, eso, ,, super, repugn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>– ¡No es no! ¡En mi cuerpo mando yo!\\r\\n– ¡Así...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>[–, ¡, No, es, no, !, ¡, En, mi, cuerpo, mando...</td>\n",
       "      <td>[–, ¡, No, es, no, !, ¡, En, mi, cuerpo, mando...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  humor  woman  \\\n",
       "index                                                                    \n",
       "9      MENTION Ya estará colocada en algún chiringuit...      0      1   \n",
       "22     Sin querer, hoy le dije “Hola” a una feminista...      1      1   \n",
       "30     En este capítulo, que se transmitió en el Cana...      0      0   \n",
       "40     MENTION MENTION MENTION Concuerdo contigo en e...      0      0   \n",
       "45     – ¡No es no! ¡En mi cuerpo mando yo!\\r\\n– ¡Así...      1      1   \n",
       "\n",
       "       lgbtiq  race  gordofobia  mean  \\\n",
       "index                                   \n",
       "9           0     0           0   3.4   \n",
       "22          0     0           0   3.8   \n",
       "30          1     0           0   2.2   \n",
       "40          1     0           0   3.8   \n",
       "45          0     0           0   2.2   \n",
       "\n",
       "                                                  tokens  \\\n",
       "index                                                      \n",
       "9      [MENTION, Ya, estará, colocada, en, algún, chi...   \n",
       "22     [Sin, querer, ,, hoy, le, dije, “, Hola, ”, a,...   \n",
       "30     [En, este, capítulo, ,, que, se, transmitió, e...   \n",
       "40     [MENTION, MENTION, MENTION, Concuerdo, contigo...   \n",
       "45     [–, ¡, No, es, no, !, ¡, En, mi, cuerpo, mando...   \n",
       "\n",
       "                                               processed  \n",
       "index                                                     \n",
       "9      [Ya, estará, colocada, en, algún, chiringuito,...  \n",
       "22     [Sin, querer, ,, hoy, le, dije, “, Hola, ”, a,...  \n",
       "30     [En, este, capítulo, ,, que, se, transmitió, e...  \n",
       "40     [Concuerdo, contigo, en, eso, ,, super, repugn...  \n",
       "45     [–, ¡, No, es, no, !, ¡, En, mi, cuerpo, mando...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text_suc_df = parsed_df.copy()\n",
    "fast_text_suc_df['processed'] = filter_tokens(fast_text_suc_df['tokens'], [Filter.SPACE, Filter.TAGS, Filter.NEWLINE])\n",
    "fast_text_suc_df['processed'] = fast_text_suc_df['processed'].apply(lambda tokens: [token.text for token in tokens])\n",
    "\n",
    "fast_text_suc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarae\\Desktop\\LNR\\LNR-project\\embeddings\\fasttext\\embeddings-l-model.vec\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'MENTION'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m FAST_TEXT_SUC_PATH \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mgetcwd(), \u001b[39m'\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfasttext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39membeddings-l-model.vec\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(FAST_TEXT_SUC_PATH)\n\u001b[1;32m----> 4\u001b[0m fast_text_suc_model \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(FAST_TEXT_SUC_PATH)\n",
      "File \u001b[1;32mc:\\Users\\sarae\\Desktop\\LNR\\LNR-project\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[0;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[0;32m   1722\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sarae\\Desktop\\LNR\\LNR-project\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2059\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2058\u001b[0m     header \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_unicode(fin\u001b[39m.\u001b[39mreadline(), encoding\u001b[39m=\u001b[39mencoding)\n\u001b[1;32m-> 2059\u001b[0m     vocab_size, vector_size \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m header\u001b[39m.\u001b[39msplit()]  \u001b[39m# throws for invalid file format\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m \u001b[39mif\u001b[39;00m limit:\n\u001b[0;32m   2061\u001b[0m     vocab_size \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(vocab_size, limit)\n",
      "File \u001b[1;32mc:\\Users\\sarae\\Desktop\\LNR\\LNR-project\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2059\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2058\u001b[0m     header \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_unicode(fin\u001b[39m.\u001b[39mreadline(), encoding\u001b[39m=\u001b[39mencoding)\n\u001b[1;32m-> 2059\u001b[0m     vocab_size, vector_size \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39;49m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m header\u001b[39m.\u001b[39msplit()]  \u001b[39m# throws for invalid file format\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m \u001b[39mif\u001b[39;00m limit:\n\u001b[0;32m   2061\u001b[0m     vocab_size \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(vocab_size, limit)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'MENTION'"
     ]
    }
   ],
   "source": [
    "# Load FastText model\n",
    "FAST_TEXT_SUC_PATH = os.path.join(os.getcwd(), 'embeddings', 'fasttext', 'embeddings-l-model.vec')\n",
    "print(FAST_TEXT_SUC_PATH)\n",
    "fast_text_suc_model = KeyedVectors.load_word2vec_format(FAST_TEXT_SUC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fast_text_suc_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fast_text_suc_vec_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(fast_text_suc_model[\u001b[39m'\u001b[39m\u001b[39mhola\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(fast_text_suc_vec_len)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fast_text_suc_model' is not defined"
     ]
    }
   ],
   "source": [
    "fast_text_suc_vec_len = len(fast_text_suc_model['hola'])\n",
    "print(fast_text_suc_vec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent each sentence as the average of its word embeddings\n",
    "def get_sentence_embedding(tokens: list[str], model):\n",
    "    embeddings = []\n",
    "    at_least_one = False\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            embeddings.append(model[token])\n",
    "            at_least_one = True\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    if not at_least_one:\n",
    "        return np.zeros(fast_text_suc_vec_len)\n",
    "    \n",
    "    return np.mean(embeddings, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fast_text_suc_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fast_text_suc_df[\u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m fast_text_suc_df[\u001b[39m'\u001b[39;49m\u001b[39mprocessed\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m tokens: get_sentence_embedding(tokens, fast_text_suc_model))\n\u001b[0;32m      2\u001b[0m display(fast_text_suc_df\u001b[39m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\sarae\\Desktop\\LNR\\LNR-project\\.venv\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\sarae\\Desktop\\LNR\\LNR-project\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\sarae\\Desktop\\LNR\\LNR-project\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\sarae\\Desktop\\LNR\\LNR-project\\.venv\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(tokens)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fast_text_suc_df[\u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m fast_text_suc_df[\u001b[39m'\u001b[39m\u001b[39mprocessed\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m tokens: get_sentence_embedding(tokens, fast_text_suc_model))\n\u001b[0;32m      2\u001b[0m display(fast_text_suc_df\u001b[39m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fast_text_suc_model' is not defined"
     ]
    }
   ],
   "source": [
    "fast_text_suc_df['embedding'] = fast_text_suc_df['processed'].apply(lambda tokens: get_sentence_embedding(tokens, fast_text_suc_model))\n",
    "display(fast_text_suc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of FastText embedding vector: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of FastText embedding vector: {len(fast_text_suc_df['embedding'].iloc[0])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Word2Vec\n",
    "Pre-trained word2vec embeddings were downloaded from [aitoralmeida/spanish_word2vec](https://github.com/aitoralmeida/spanish_word2vec).\n",
    "\n",
    "Aitor Almeida, & Aritz Bilbao. (2018). Spanish 3B words Word2Vec Embeddings (Version 1.0) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.1410403\n",
    "Bilbao-Jayo, A., & Almeida, A. (2018). Automatic political discourse analysis with multi-scale convolutional neural networks and contextual data. International Journal of Distributed Sensor Networks, 14(11), 1550147718811827."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings\n",
    "word2vec_path = os.path.join(os.getcwd(), 'embeddings', 'word2vec', 'complete.kv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: GloVe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
